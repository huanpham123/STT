<!DOCTYPE html>
<html lang="vi">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>Demo STT (Flask + speech_recognition)</title>

  <style>
    body {
      font-family: Arial, sans-serif;
      margin: 20px;
    }
    h1 {
      font-size: 24px;
      margin-bottom: 10px;
    }
    #controls {
      margin-bottom: 15px;
    }
    #record-btn {
      padding: 10px 20px;
      font-size: 16px;
      border: none;
      border-radius: 4px;
      background-color: #007bff;
      color: white;
      cursor: pointer;
    }
    #record-btn.recording {
      background-color: #dc3545;
    }
    #status {
      margin-left: 10px;
      font-weight: bold;
    }
    #transcript {
      margin-top: 20px;
      font-size: 18px;
      color: #333;
      white-space: pre-wrap;
    }
  </style>
</head>
<body>
  <h1>Demo STT (Flask + speech_recognition) tr√™n Vercel</h1>

  <div id="controls">
    <button id="record-btn">üé§ B·∫Øt ƒë·∫ßu ghi √¢m</button>
    <span id="status">Ch∆∞a ghi √¢m</span>
  </div>

  <div id="transcript">B·∫£n phi√™n √¢m s·∫Ω hi·ªán ·ªü ƒë√¢y ‚Ä¶</div>

  <script>
    // --- To√†n c·ª•c ƒë·ªÉ ghi √¢m v√† chuy·ªÉn WAV ---
    let audioContext;
    let micStream;
    let scriptProcessor;
    let bufferData = [];
    const TARGET_SAMPLE_RATE = 16000; // 16kHz, ph√π h·ª£p speech_recognition

    const recordBtn = document.getElementById("record-btn");
    const statusSpan = document.getElementById("status");
    const transcriptDiv = document.getElementById("transcript");
    let isRecording = false;

    recordBtn.addEventListener("click", () => {
      if (!isRecording) {
        startRecording();
      } else {
        stopRecording();
      }
    });

    async function startRecording() {
      try {
        // 1) Xin quy·ªÅn mic
        const stream = await navigator.mediaDevices.getUserMedia({ audio: true });
        audioContext = new (window.AudioContext || window.webkitAudioContext)();
        micStream = audioContext.createMediaStreamSource(stream);

        // 2) T·∫°o ScriptProcessor ƒë·ªÉ l·∫•y PCM Float32
        scriptProcessor = audioContext.createScriptProcessor(4096, 1, 1);
        micStream.connect(scriptProcessor);
        scriptProcessor.connect(audioContext.destination);

        bufferData = []; // X√≥a buffer

        scriptProcessor.onaudioprocess = event => {
          const input = event.inputBuffer.getChannelData(0); // Float32Array
          const downsampled = downsampleBuffer(input, audioContext.sampleRate, TARGET_SAMPLE_RATE);
          if (downsampled) {
            bufferData.push(downsampled);
          }
        };

        isRecording = true;
        recordBtn.textContent = "‚ñ† D·ª´ng ghi √¢m";
        recordBtn.classList.add("recording");
        statusSpan.textContent = "ƒêang ghi √¢m...";
        transcriptDiv.textContent = "";
      } catch (err) {
        console.error("Kh√¥ng th·ªÉ truy c·∫≠p microphone:", err);
        alert("Kh√¥ng th·ªÉ truy c·∫≠p microphone: " + err);
      }
    }

    function stopRecording() {
      // 1) Ng·∫Øt processor v√† mic
      scriptProcessor.disconnect();
      micStream.disconnect();
      audioContext.close();

      isRecording = false;
      recordBtn.textContent = "üé§ B·∫Øt ƒë·∫ßu ghi √¢m";
      recordBtn.classList.remove("recording");
      statusSpan.textContent = "ƒêang x·ª≠ l√Ω‚Ä¶";

      // 2) G·ªôp bufferData th√†nh WAV Blob
      const wavBlob = encodeWAV(bufferData, TARGET_SAMPLE_RATE);
      // 3) G·ª≠i l√™n /transcribe
      sendToServer(wavBlob);
    }

    // H√†m downsample Float32Array t·ª´ inputRate ‚Üí outputRate,
    // tr·∫£ v·ªÅ Int16Array
    function downsampleBuffer(buffer, inputRate, outputRate) {
      if (outputRate === inputRate) {
        const int16 = new Int16Array(buffer.length);
        for (let i = 0; i < buffer.length; i++) {
          let s = Math.max(-1, Math.min(1, buffer[i]));
          int16[i] = s < 0 ? s * 0x8000 : s * 0x7FFF;
        }
        return int16;
      }
      const ratio = inputRate / outputRate;
      const newLength = Math.round(buffer.length / ratio);
      const result = new Int16Array(newLength);
      let offsetResult = 0;
      let offsetBuffer = 0;
      while (offsetResult < newLength) {
        const nextOffsetBuffer = Math.round((offsetResult + 1) * ratio);
        let sum = 0, count = 0;
        for (let i = offsetBuffer; i < nextOffsetBuffer && i < buffer.length; i++) {
          sum += buffer[i];
          count++;
        }
        const avg = sum / count;
        result[offsetResult] = avg < 0 ? avg * 0x8000 : avg * 0x7FFF;
        offsetResult++;
        offsetBuffer = nextOffsetBuffer;
      }
      return result;
    }

    // H√†m t·∫°o WAV Blob t·ª´ m·∫£ng Int16Array[]
    function encodeWAV(buffers, sampleRate) {
      let totalSamples = 0;
      for (let i = 0; i < buffers.length; i++) {
        totalSamples += buffers[i].length;
      }
      const bytesPerSample = 2;
      const blockAlign = 1 * bytesPerSample;
      const byteRate = sampleRate * blockAlign;
      const dataSize = totalSamples * bytesPerSample;
      const buffer = new ArrayBuffer(44 + dataSize);
      const view = new DataView(buffer);

      // RIFF header
      writeString(view, 0, "RIFF");
      view.setUint32(4, 36 + dataSize, true);
      writeString(view, 8, "WAVE");
      // fmt chunk
      writeString(view, 12, "fmt ");
      view.setUint32(16, 16, true);      // Subchunk1Size = 16 (PCM)
      view.setUint16(20, 1, true);       // AudioFormat = 1 (PCM)
      view.setUint16(22, 1, true);       // NumChannels = 1
      view.setUint32(24, sampleRate, true);
      view.setUint32(28, byteRate, true);
      view.setUint16(32, blockAlign, true);
      view.setUint16(34, 16, true);      // BitsPerSample = 16
      // data chunk
      writeString(view, 36, "data");
      view.setUint32(40, dataSize, true);

      // Ghi PCM samples
      let offset = 44;
      for (let i = 0; i < buffers.length; i++) {
        const buf = buffers[i];
        for (let j = 0; j < buf.length; j++, offset += 2) {
          view.setInt16(offset, buf[j], true);
        }
      }
      return new Blob([view], { type: "audio/wav" });
    }

    function writeString(view, offset, str) {
      for (let i = 0; i < str.length; i++) {
        view.setUint8(offset + i, str.charCodeAt(i));
      }
    }

    function sendToServer(wavBlob) {
      const fd = new FormData();
      fd.append("audio_data", wavBlob, "recording.wav");

      fetch("/transcribe", {
        method: "POST",
        body: fd
      })
      .then(resp => resp.json())
      .then(data => {
        if (data.transcript !== undefined) {
          transcriptDiv.textContent = data.transcript;
          statusSpan.textContent = "Ho√†n t·∫•t.";
        } else if (data.error) {
          transcriptDiv.textContent = "L·ªói: " + data.error;
          statusSpan.textContent = "L·ªói.";
        } else {
          transcriptDiv.textContent = "Kh√¥ng nh·∫≠n ƒë∆∞·ª£c transcript.";
          statusSpan.textContent = "Xong.";
        }
      })
      .catch(err => {
        console.error("L·ªói g·ª≠i l√™n server:", err);
        transcriptDiv.textContent = "L·ªói khi g·ªçi /transcribe: " + err;
        statusSpan.textContent = "L·ªói.";
      });
    }
  </script>
</body>
</html>
